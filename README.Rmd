---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# easyllm

<!-- badges: start -->
<!-- badges: end -->

The goal of easyllm is to provide easy access to large language models from R (e.g., via LMStudio's local server feature or OpenAI's web API).

## Installation

You can install the development version of easyllm from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("affcomlab/easyllm")
```

## LMStudio's Local Server Example

After setting up LMStudio and loading the Llama3 model described below into your local inference server, use the following code to prompt it:

```{r example1, cache=TRUE}
library(easyllm)
response1 <- ask_lmstudio(
  model = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF",
  user_messages = "Introduce yourself.",
  temperature = 0
)
writeLines(response1)
```

```{r example2, cache=TRUE}
response2 <- ask_lmstudio(
  model = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF",
  user_messages = "Introduce yourself.",
  system_message = "Always answer in rhymes.",
  temperature = 0
)
writeLines(response2)
```

## OpenAI's Web API Example

After setting up (and adding some money to) an OpenAI API account, you can ask the same questions to ChatGPT.

```{r api_key, eval=FALSE}
save_key(key = "PASTE-KEY-HERE", name = "openai")
```

```{r example3, cache=TRUE}
response3 <- ask_openai(
  model = "gpt-4o-mini",
  api_key = load_key("openai"),
  user_messages = "Introduce yourself.",
  temperature = 0
)
writeLines(response3)
```

```{r example4, cache=TRUE}
response4 <- ask_openai(
  model = "gpt-4o-mini",
  api_key = load_key("openai"),
  user_messages = "Introduce yourself.",
  system_message = "Always answer in rhymes.",
  temperature = 0
)
writeLines(response4)
```
