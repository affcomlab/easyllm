---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# lmprompt

<!-- badges: start -->
<!-- badges: end -->

The goal of lmprompt is to provide easy access to LLMs from R.

## Installation

You can install the development version of lmprompt from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("affcomlab/lmprompt")
```

## Example

After setting up LMStudio and loading the Llama3 model described below into your local inference server, use the following code to prompt it:

```{r example1, cache=TRUE}
library(lmprompt)
response1 <- ask_lmstudio(
  model = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF",
  user_messages = "Introduce yourself.",
  temperature = 0
)
writeLines(response1)
```

```{r example2, cache=TRUE}
response2 <- ask_lmstudio(
  model = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF",
  user_messages = "Introduce yourself.",
  system_message = "Always answer in rhymes.",
  temperature = 0
)
writeLines(response2)
```

Or, after setting up (and adding some money to) an OpenAI API account, you can ask the same questions to ChatGPT.

```{r example3, cache=TRUE}
response3 <- ask_openai(
  model = "gpt-4o-mini",
  api_key = load_key("openai"),
  user_messages = "Introduce yourself."
)
writeLines(response3)
```

```{r example4, cache=TRUE}
response4 <- ask_openai(
  model = "gpt-4o-mini",
  api_key = load_key("openai"),
  user_messages = "Introduce yourself.",
  system_message = "Always answer in rhymes.",
)
writeLines(response4)
```
